# Deep_Learning_Schoool_MFTI_Part2_NLP_2025
# Natural Language Processing (NLP)

This repository contains materials for **Part 2: Natural Language Processing (NLP)** — a course focused on modern deep learning approaches to natural language processing.

The course covers fundamental and state-of-the-art NLP architectures, including **recurrent neural networks**, **attention mechanisms**, and **transformers**, as well as modern approaches to building and using **large language models**.

Students will work on key NLP tasks such as **text classification**, **language modeling**, and **machine translation**, studying both classical and modern solutions. Special attention is given to **pretraining and fine-tuning** language models, **GPT-like architectures**, **zero-shot learning**, and emerging directions such as **Retrieval-Augmented Generation (RAG)**, **model interpretability**, and **AI-generated text detection**.

## Technologies and Tools

The course is based on **Python** and **PyTorch**, with practical work using:

* PyTorch
* Hugging Face Transformers
* NumPy
* Scikit-learn
* Pandas
* Matplotlib

Experiments and assignments are conducted in **Jupyter Notebooks** and **Google Colab**.

## Course Structure

The course is divided into **three thematic blocks** and runs for a total of **10 weeks**:

* **Block 1 (4 weeks): Fundamentals of NLP and Recurrent Models**
* **Block 2 (4 weeks): Modern Architectures and Language Models**
* **Block 3 (2 weeks): Advanced Topics and Practice**

## Modules Overview

### Block 1. Fundamentals of NLP and Recurrent Models

#### Module 1. Introduction to NLP and Word Embeddings

Introduction to core NLP concepts and methods for representing words as embeddings.
**Assignment:** Text ranking based on word embeddings.

#### Module 2. Recurrent Neural Networks (RNN)

Study recurrent architectures (RNN, LSTM, GRU) and their application to text classification tasks.
**Assignment:** Text classification using RNN-based models.

#### Module 3. Language Modeling

Overview of classical and neural approaches to language modeling, evaluation metrics, and text generation methods. Practical training of word-level language models.
**Assignment:** Building a language model using RNNs.

#### Module 4. Machine Translation and Attention

Machine translation from basic encoder–decoder architectures to attention mechanisms that significantly improve translation quality.

---

### Block 2. Modern Architectures and Language Models

#### Module 5. Transformer

In-depth study of the Transformer architecture — an attention-based model that revolutionized NLP and became the industry standard.

#### Module 6. Pretraining and Fine-Tuning of Language Models

Modern language model pretraining strategies and fine-tuning techniques for downstream tasks.
**Assignment:** Fine-tuning a pretrained language model for text classification.

#### Module 7. From GPT to GPT-3. Zero-Shot Learning

Evolution of GPT-style models from early versions to GPT-3. Introduction to zero-shot learning and its practical applications.

#### Module 8. GPT-2, GPT-3, and Retrieval-Augmented Generation (RAG)

Further exploration of GPT-family models and introduction to the RAG paradigm.
**Assignment:** Implementing a Retrieval-Augmented Generation model.

---

### Block 3. Advanced Topics and Practice

#### Module 9. Interpretability of Transformers

Methods for interpreting transformer models, including SAE, attention-based analysis, model circuits, and techniques for understanding internal representations.

#### Module 10. Detection of AI-Generated Text

Modern approaches to detecting text generated by neural language models and the challenges associated with this task.
**Assignment:** Detection of AI-generated text.


---
references:
To enroll in **Part 2: Natural Language Processing (NLP)**, please refer to the official course page.
https://dls.samcs.ru/part2
* сделать версию **для учебного репозитория с ноутбуками и ДЗ**,
* или адаптировать текст под **публичный vs приватный GitHub**.
